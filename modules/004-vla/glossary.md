# Module 4: Vision-Language-Action (VLA) Glossary

## A

**ASR (Automatic Speech Recognition)**: Technology that converts spoken language into text.

**API (Application Programming Interface)**: A set of rules and protocols for building and interacting with software applications.

## C

**CLIP (Contrastive Language-Image Pre-training)**: A neural network that learns visual concepts from natural language supervision.

## L

**LLM (Large Language Model)**: A language model with a large number of parameters that can generate human-like text and understand complex queries.

## O

**OpenAI Whisper**: An automatic speech recognition (ASR) system developed by OpenAI that converts speech to text.

## R

**ROS 2 (Robot Operating System 2)**: A flexible framework for writing robot software that provides hardware abstraction, device drivers, libraries, and more.

## V

**VLA (Vision-Language-Action)**: A system that integrates visual perception, natural language understanding, and physical action for autonomous robot behavior.

**Vision Processing**: The analysis and interpretation of visual data from cameras to understand the environment.

**Voice Processing**: The conversion and interpretation of spoken language into actionable commands.

**Vision-Language Integration**: The combination of visual perception and language understanding to enable multimodal robot interaction.